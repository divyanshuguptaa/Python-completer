diff --git a/.gitignore b/.gitignore
index 37299d1..b2dc025 100644
--- a/.gitignore
+++ b/.gitignore
@@ -4,4 +4,4 @@ data/
 __pycache__/
 .DS_Store
 .*.swp
-lab
+logs
diff --git a/.lab.yaml b/.lab.yaml
index e69de29..1e5d0a2 100644
--- a/.lab.yaml
+++ b/.lab.yaml
@@ -0,0 +1 @@
+check_repo_dirty: False
\ No newline at end of file
diff --git a/evaluate.py b/evaluate.py
index 559cf62..1e66630 100644
--- a/evaluate.py
+++ b/evaluate.py
@@ -6,25 +6,20 @@ from typing import NamedTuple, List, Tuple
 
 import torch
 import torch.nn
+from lab import experiment, monit, logger
+from lab.logger import Text, Style
 
 import parser.load
 import parser.tokenizer
-from lab import colors
-from lab.experiment.pytorch import Experiment
 from model import SimpleLstmModel
 from parser import tokenizer
 
 # Experiment configuration to load checkpoints
-EXPERIMENT = Experiment(name="simple_lstm",
-                        python_file=__file__,
-                        comment="Simple LSTM",
-                        check_repo_dirty=False,
-                        is_log_python_file=False)
-
-logger = EXPERIMENT.logger
+experiment.create(name="simple_lstm",
+                  comment="Simple LSTM")
 
 # device to evaluate on
-device = torch.device("cuda:1")
+device = torch.device("cuda:0")
 
 # Beam search
 BEAM_SIZE = 8
@@ -349,16 +344,13 @@ class Evaluator:
     def eval(self):
         keys_saved = 0
 
-        logger.info(total_keys=sum([len(c) for c in self.__content]),
-                    total_lines=len(self.__content))
-
         for line, content in enumerate(self.__content):
             # Keep reference to rest of the line
             rest_of_line = content
 
             # Build the line for logging with colors
             # The line number
-            logs = [(f"{line: 4d}: ", colors.BrightColor.cyan)]
+            logs = [(f"{line: 4d}: ", Text.meta)]
 
             # Type the line character by character
             while rest_of_line != '':
@@ -367,8 +359,8 @@ class Evaluator:
                 # If suggestion matches
                 if suggestion != '' and rest_of_line.startswith(suggestion):
                     # Log
-                    logs.append((suggestion[0], colors.BrightColor.green))
-                    logs.append((suggestion[1:], colors.BrightBackground.black))
+                    logs.append((suggestion[0], Text.danger))
+                    logs.append((suggestion[1:], Style.underline))
 
                     keys_saved += len(suggestion) - 1
 
@@ -382,40 +374,45 @@ class Evaluator:
                 else:
                     # Add the next character
                     self.__predictor.add(rest_of_line[0])
-                    logs.append((rest_of_line[0], None))
+                    logs.append((rest_of_line[0], Text.subtle))
                     rest_of_line = rest_of_line[1:]
 
             # Add a new line
             self.__predictor.add("\n")
 
             # Log the line
-            logger.log_color(logs)
+            logger.log(logs)
 
         # Log time taken for the file
-        logger.info(add=self.__predictor.time_add,
-                    check=self.__predictor.time_check,
-                    predict=self.__predictor.time_predict)
-        return keys_saved
+        logger.inspect(add=self.__predictor.time_add,
+                       check=self.__predictor.time_check,
+                       predict=self.__predictor.time_predict)
+
+        total_keys = sum([len(c) for c in self.__content])
+        logger.inspect(keys_saved=keys_saved,
+                       percentage_saved=100 * keys_saved / total_keys,
+                       total_keys=total_keys,
+                       total_lines=len(self.__content))
 
 
 def main():
     lstm_size = 1024
     lstm_layers = 3
 
-    with logger.section("Loading data"):
+    with monit.section("Loading data"):
         files = parser.load.load_files()
         train_files, valid_files = parser.load.split_train_valid(files, is_shuffle=False)
 
-    with logger.section("Create model"):
+    with monit.section("Create model"):
         model = SimpleLstmModel(encoding_size=tokenizer.VOCAB_SIZE,
                                 embedding_size=tokenizer.VOCAB_SIZE,
                                 lstm_size=lstm_size,
                                 lstm_layers=lstm_layers)
         model.to(device)
 
-    EXPERIMENT.add_models({'base': model})
+    experiment.add_pytorch_models({'base': model})
 
-    EXPERIMENT.start_replay()
+    experiment.load("b3000660936b11ea9aa1c9a10ca3c0a4")
 
     # For debugging with a specific piece of source code
     # predictor = Predictor(model, lstm_layers, lstm_size)
@@ -425,13 +422,11 @@ def main():
 
     # Evaluate all the files in validation set
     for file in valid_files:
-        logger.log(str(file.path), color=colors.BrightColor.orange)
+        logger.log(str(file.path), Text.heading)
         evaluator = Evaluator(model, file,
                               lstm_layers, lstm_size,
                               skip_spaces=True)
-        keys_saved = evaluator.eval()
-
-        logger.info(keys_saved=keys_saved)
+        evaluator.eval()
 
 
 if __name__ == '__main__':
diff --git a/extract_code.py b/extract_code.py
old mode 100644
new mode 100755
index 6eee33e..70ed0ab
--- a/extract_code.py
+++ b/extract_code.py
@@ -7,15 +7,14 @@ import os
 from pathlib import Path
 from typing import List, NamedTuple
 
-from lab.logger import Logger
+from lab import logger, monit
+
 from parser import tokenizer
 from parser.tokenizer import encode, parse_string
 
 COMMENT = '#'
 MULTI_COMMENT = '"""'
 
-_logger = Logger()
-
 
 class _PythonFile(NamedTuple):
     relative_path: str
@@ -27,12 +26,13 @@ class _GetPythonFiles:
     """
     Get list of python files and their paths inside `data/source` folder
     """
+
     def __init__(self):
         self.source_path = Path(os.getcwd()) / 'data' / 'source'
         self.files: List[_PythonFile] = []
         self.get_python_files(self.source_path)
 
-        _logger.info([f.path for f in self.files])
+        logger.inspect([f.path for f in self.files])
 
     def add_file(self, path: Path):
         """
@@ -151,17 +151,15 @@ def _read_file(path: Path) -> List[int]:
 def main():
     source_files = _GetPythonFiles().files
 
-    _logger.info(source_files)
+    logger.inspect(source_files)
 
     with open(str(Path(os.getcwd()) / 'data' / 'all.py'), 'w') as f:
-        with _logger.section("Parse", total_steps=len(source_files)):
-            for i, source in enumerate(source_files):
-                serialized = _read_file(source.path)
-                # return
-                serialized = [str(t) for t in serialized]
-                f.write(f"{str(source.path)}\n")
-                f.write(" ".join(serialized) + "\n")
-                _logger.progress(i + 1)
+        for i, source in monit.enum("Parse", source_files):
+            serialized = _read_file(source.path)
+            # return
+            serialized = [str(t) for t in serialized]
+            f.write(f"{str(source.path)}\n")
+            f.write(" ".join(serialized) + "\n")
 
 
 if __name__ == '__main__':
diff --git a/logs/simple_lstm/checkpoints/536410/base_embedding.weight.npy b/logs/simple_lstm/checkpoints/536410/base_embedding.weight.npy
deleted file mode 100644
index 31d17b4..0000000
Binary files a/logs/simple_lstm/checkpoints/536410/base_embedding.weight.npy and /dev/null differ
diff --git a/logs/simple_lstm/checkpoints/536410/base_fc.bias.npy b/logs/simple_lstm/checkpoints/536410/base_fc.bias.npy
deleted file mode 100644
index 775c839..0000000
Binary files a/logs/simple_lstm/checkpoints/536410/base_fc.bias.npy and /dev/null differ
diff --git a/logs/simple_lstm/checkpoints/536410/base_fc.weight.npy b/logs/simple_lstm/checkpoints/536410/base_fc.weight.npy
deleted file mode 100644
index 8bfe8de..0000000
Binary files a/logs/simple_lstm/checkpoints/536410/base_fc.weight.npy and /dev/null differ
diff --git a/logs/simple_lstm/checkpoints/536410/base_lstm.bias_hh_l0.npy b/logs/simple_lstm/checkpoints/536410/base_lstm.bias_hh_l0.npy
deleted file mode 100644
index a36c62c..0000000
Binary files a/logs/simple_lstm/checkpoints/536410/base_lstm.bias_hh_l0.npy and /dev/null differ
diff --git a/logs/simple_lstm/checkpoints/536410/base_lstm.bias_hh_l1.npy b/logs/simple_lstm/checkpoints/536410/base_lstm.bias_hh_l1.npy
deleted file mode 100644
index bd257e9..0000000
Binary files a/logs/simple_lstm/checkpoints/536410/base_lstm.bias_hh_l1.npy and /dev/null differ
diff --git a/logs/simple_lstm/checkpoints/536410/base_lstm.bias_hh_l2.npy b/logs/simple_lstm/checkpoints/536410/base_lstm.bias_hh_l2.npy
deleted file mode 100644
index f6fda59..0000000
Binary files a/logs/simple_lstm/checkpoints/536410/base_lstm.bias_hh_l2.npy and /dev/null differ
diff --git a/logs/simple_lstm/checkpoints/536410/base_lstm.bias_ih_l0.npy b/logs/simple_lstm/checkpoints/536410/base_lstm.bias_ih_l0.npy
deleted file mode 100644
index 60674f9..0000000
Binary files a/logs/simple_lstm/checkpoints/536410/base_lstm.bias_ih_l0.npy and /dev/null differ
diff --git a/logs/simple_lstm/checkpoints/536410/base_lstm.bias_ih_l1.npy b/logs/simple_lstm/checkpoints/536410/base_lstm.bias_ih_l1.npy
deleted file mode 100644
index 618cc33..0000000
Binary files a/logs/simple_lstm/checkpoints/536410/base_lstm.bias_ih_l1.npy and /dev/null differ
diff --git a/logs/simple_lstm/checkpoints/536410/base_lstm.bias_ih_l2.npy b/logs/simple_lstm/checkpoints/536410/base_lstm.bias_ih_l2.npy
deleted file mode 100644
index ba3eeb7..0000000
Binary files a/logs/simple_lstm/checkpoints/536410/base_lstm.bias_ih_l2.npy and /dev/null differ
diff --git a/logs/simple_lstm/checkpoints/536410/base_lstm.weight_hh_l0.npy b/logs/simple_lstm/checkpoints/536410/base_lstm.weight_hh_l0.npy
deleted file mode 100644
index ac0f9ac..0000000
Binary files a/logs/simple_lstm/checkpoints/536410/base_lstm.weight_hh_l0.npy and /dev/null differ
diff --git a/logs/simple_lstm/checkpoints/536410/base_lstm.weight_hh_l1.npy b/logs/simple_lstm/checkpoints/536410/base_lstm.weight_hh_l1.npy
deleted file mode 100644
index 773440f..0000000
Binary files a/logs/simple_lstm/checkpoints/536410/base_lstm.weight_hh_l1.npy and /dev/null differ
diff --git a/logs/simple_lstm/checkpoints/536410/base_lstm.weight_hh_l2.npy b/logs/simple_lstm/checkpoints/536410/base_lstm.weight_hh_l2.npy
deleted file mode 100644
index ea331c0..0000000
Binary files a/logs/simple_lstm/checkpoints/536410/base_lstm.weight_hh_l2.npy and /dev/null differ
diff --git a/logs/simple_lstm/checkpoints/536410/base_lstm.weight_ih_l0.npy b/logs/simple_lstm/checkpoints/536410/base_lstm.weight_ih_l0.npy
deleted file mode 100644
index 467f734..0000000
Binary files a/logs/simple_lstm/checkpoints/536410/base_lstm.weight_ih_l0.npy and /dev/null differ
diff --git a/logs/simple_lstm/checkpoints/536410/base_lstm.weight_ih_l1.npy b/logs/simple_lstm/checkpoints/536410/base_lstm.weight_ih_l1.npy
deleted file mode 100644
index 631e007..0000000
Binary files a/logs/simple_lstm/checkpoints/536410/base_lstm.weight_ih_l1.npy and /dev/null differ
diff --git a/logs/simple_lstm/checkpoints/536410/base_lstm.weight_ih_l2.npy b/logs/simple_lstm/checkpoints/536410/base_lstm.weight_ih_l2.npy
deleted file mode 100644
index a9730e1..0000000
Binary files a/logs/simple_lstm/checkpoints/536410/base_lstm.weight_ih_l2.npy and /dev/null differ
diff --git a/logs/simple_lstm/checkpoints/536410/info.json b/logs/simple_lstm/checkpoints/536410/info.json
deleted file mode 100644
index 3e172d3..0000000
--- a/logs/simple_lstm/checkpoints/536410/info.json
+++ /dev/null
@@ -1 +0,0 @@
-{"base": {"embedding.weight": "base_embedding.weight.npy", "lstm.weight_ih_l0": "base_lstm.weight_ih_l0.npy", "lstm.weight_hh_l0": "base_lstm.weight_hh_l0.npy", "lstm.bias_ih_l0": "base_lstm.bias_ih_l0.npy", "lstm.bias_hh_l0": "base_lstm.bias_hh_l0.npy", "lstm.weight_ih_l1": "base_lstm.weight_ih_l1.npy", "lstm.weight_hh_l1": "base_lstm.weight_hh_l1.npy", "lstm.bias_ih_l1": "base_lstm.bias_ih_l1.npy", "lstm.bias_hh_l1": "base_lstm.bias_hh_l1.npy", "lstm.weight_ih_l2": "base_lstm.weight_ih_l2.npy", "lstm.weight_hh_l2": "base_lstm.weight_hh_l2.npy", "lstm.bias_ih_l2": "base_lstm.bias_ih_l2.npy", "lstm.bias_hh_l2": "base_lstm.bias_hh_l2.npy", "fc.weight": "base_fc.weight.npy", "fc.bias": "base_fc.bias.npy"}}
\ No newline at end of file
diff --git a/model.py b/model.py
index c736cd5..5e02eb9 100644
--- a/model.py
+++ b/model.py
@@ -1,7 +1,7 @@
-import torch.nn
+from torch import nn
 
 
-class SimpleLstmModel(torch.nn.Module):
+class SimpleLstmModel(nn.Module):
     def __init__(self, *,
                  encoding_size,
                  embedding_size,
@@ -9,12 +9,12 @@ class SimpleLstmModel(torch.nn.Module):
                  lstm_layers):
         super().__init__()
 
-        self.embedding = torch.nn.Embedding(encoding_size, embedding_size)
-        self.lstm = torch.nn.LSTM(input_size=embedding_size,
-                                  hidden_size=lstm_size,
-                                  num_layers=lstm_layers)
-        self.fc = torch.nn.Linear(lstm_size, encoding_size)
-        self.softmax = torch.nn.Softmax(dim=-1)
+        self.embedding = nn.Embedding(encoding_size, embedding_size)
+        self.lstm = nn.LSTM(input_size=embedding_size,
+                            hidden_size=lstm_size,
+                            num_layers=lstm_layers)
+        self.fc = nn.Linear(lstm_size, encoding_size)
+        self.softmax = nn.Softmax(dim=-1)
 
     def forward(self, x, h0, c0):
         # shape of x is [seq, batch, feat]
diff --git a/parser/load.py b/parser/load.py
index 70b1d92..43892ff 100644
--- a/parser/load.py
+++ b/parser/load.py
@@ -2,12 +2,10 @@ from pathlib import Path
 from typing import NamedTuple, List
 
 import numpy as np
+from lab import logger
 
-from lab.logger import Logger
 from parser import tokenizer
 
-logger = Logger()
-
 
 class EncodedFile(NamedTuple):
     path: str
@@ -34,7 +32,8 @@ def load_files() -> List[EncodedFile]:
     return files
 
 
-def split_train_valid(files: List[EncodedFile], is_shuffle=True) -> (List[EncodedFile], List[EncodedFile]):
+def split_train_valid(files: List[EncodedFile],
+                      is_shuffle=True) -> (List[EncodedFile], List[EncodedFile]):
     """
     Split training and validation sets
     """
@@ -55,15 +54,15 @@ def split_train_valid(files: List[EncodedFile], is_shuffle=True) -> (List[Encode
     if train_size < total_size * 0.60:
         raise RuntimeError("Validation set too large")
 
-    logger.info(train_size=train_size,
-                valid_size=valid_size,
-                vocab=tokenizer.VOCAB_SIZE)
+    logger.inspect(train_size=train_size,
+                   valid_size=valid_size,
+                   vocab=tokenizer.VOCAB_SIZE)
     return files, valid
 
 
 def main():
-    files, code_to_str = load_files()
-    logger.info(code_to_str)
+    files = load_files()
+    logger.inspect(files)
 
 
 if __name__ == "__main__":
diff --git a/parser/tokenizer.py b/parser/tokenizer.py
index 806845f..1fde875 100644
--- a/parser/tokenizer.py
+++ b/parser/tokenizer.py
@@ -1,6 +1,8 @@
 import tokenize
 from io import BytesIO
-from typing import Optional, List, NamedTuple
+from typing import Optional, List, NamedTuple, Union
+
+import numpy as np
 
 
 class TokenType:
@@ -33,6 +35,7 @@ class _TokenParser:
     """
     Parse tokens
     """
+
     def __init__(self, token_type, tokenize_type, match_type, values,
                  replacement=None):
         self.offset = 0
@@ -55,32 +58,36 @@ class _TokenParser:
         """
         Parse token
         """
-        if type(self.tokenize_type) == list:
-            if token.type not in self.tokenize_type:
-                return None
-        else:
-            if token.type != self.tokenize_type:
+        try:
+            if type(self.tokenize_type) == list:
+                if token.type not in self.tokenize_type:
+                    return None
+            else:
+                if token.type != self.tokenize_type:
+                    return None
+
+            # Perhaps use subclasses?
+            if self.match_type == _MatchType.exact:
+                if token.string not in self.match_set:
+                    return None
+                return [ParsedToken(self.token_type, self.match_set[token.string])]
+            elif self.match_type == _MatchType.each:
+                res = []
+                for ch in token.string:
+                    res.append(ParsedToken(self.token_type, self.match_set[ch]))
+                return res
+            elif self.match_type == _MatchType.starts:
+                for i, pref in enumerate(self.values):
+                    if token.string.startswith(pref):
+                        return [ParsedToken(self.token_type, i)]
                 return None
-
-        # Perhaps use subclasses?
-        if self.match_type == _MatchType.exact:
-            if token.string not in self.match_set:
-                return None
-            return [ParsedToken(self.token_type, self.match_set[token.string])]
-        elif self.match_type == _MatchType.each:
-            res = []
-            for ch in token.string:
-                res.append(ParsedToken(self.token_type, self.match_set[ch]))
-            return res
-        elif self.match_type == _MatchType.starts:
-            for i, pref in enumerate(self.values):
-                if token.string.startswith(pref):
-                    return [ParsedToken(self.token_type, i)]
-            return None
-        elif self.match_type == _MatchType.none:
-            return [ParsedToken(self.token_type, 0)]
-        else:
-            raise RuntimeError(self.match_type)
+            elif self.match_type == _MatchType.none:
+                return [ParsedToken(self.token_type, 0)]
+            else:
+                raise RuntimeError(self.match_type)
+        except Exception as e:
+            print(token)
+            raise e
 
     def calc_serialize_range(self):
         for p in _PARSERS:
@@ -103,7 +110,7 @@ _CHARS += [chr(i + ord('a')) for i in range(26)]
 _CHARS += [chr(i + ord('A')) for i in range(26)]
 _CHARS += [chr(i + ord('0')) for i in range(10)]
 
-_NUMS = ['.', '_', 'e', 'x', '-']
+_NUMS = ['.', '_', 'x', 'X', 'o', 'O', '-', '+', 'j', 'J']
 _NUMS += [chr(i + ord('a')) for i in range(6)]
 _NUMS += [chr(i + ord('A')) for i in range(6)]
 _NUMS += [chr(i + ord('0')) for i in range(10)]
@@ -120,7 +127,7 @@ _PARSERS = [
                   '&', '|', '^', '~', '<<', '>>',
                   '&=', '|=', '^=', '~=', '<<=', '>>=',
                   '.', ',', '(', ')', ':', '[', ']', '{', '}',
-                  '@', '...', ';']),
+                  '@', '...', ';', '->']),
     _TokenParser(TokenType.keyword, tokenize.NAME, _MatchType.exact,
                  ['and', 'as', 'assert', 'break', 'class',
                   'continue', 'def', 'del', 'elif', 'else',
@@ -138,10 +145,19 @@ _PARSERS = [
     _TokenParser(TokenType.comment, tokenize.COMMENT, _MatchType.none, '#')
 ]
 
+
+def get_vocab_size(token_type: int):
+    return len(_PARSERS[token_type])
+
+
+def get_vocab_offset(token_type: int):
+    return _PARSERS[token_type].offset
+
+
 VOCAB_SIZE = 0
-DECODE = []
-LENGTHS = []
-DESERIALIZE = []
+DECODE: List[List[str]] = []
+LENGTHS: List[int] = []
+DESERIALIZE: List[ParsedToken] = []
 
 SKIP_TOKENS = {tokenize.ENCODING, tokenize.ENDMARKER}
 EMPTY_TOKENS = {TokenType.eof, TokenType.new_line, TokenType.indent, TokenType.dedent}
@@ -238,7 +254,7 @@ def encode(tokens: List[ParsedToken]) -> List[int]:
     return [_encode_token(t) for t in tokens]
 
 
-def decode(codes: List[int]) -> List[ParsedToken]:
+def decode(codes: Union[np.ndarray, List[int]]) -> List[ParsedToken]:
     """
     Decode codes to tokens
     """
diff --git a/readme.md b/readme.md
index 8efedba..f408715 100644
--- a/readme.md
+++ b/readme.md
@@ -1,15 +1,32 @@
-[This](https://github.com/vpj/python_autocomplete) a toy project we started to see how well a simple LSTM model can autocomplete python code.
-
-It gives quite decent results by saving above 30% key strokes in most files, and close to 50% in some. We calculated key strokes saved by making a single (best) prediction and selecting it with a single key.
-
-We do a beam search to find predictions, upto ~10 characters ahead. So far it's too inefficient, if you are wondering about editor integration.
-
-We train and predict on after cleaning comments, strings and blank lines in python code.
-The model is trained after tokenizing python code. It seems more efficient than character level prediction with byte-pair encoding.
-
-A saved model is included in this repo. It is trained on [tensorflow/models](https://github.com/tensorflow/models).
-
-Here's a sample evaluation on a source file from validation set. Green characters are when a autocompletion started; i.e. user presses TAB to select the completion. The green character and and the following characters highlighted in gray are autocompleted. As you can see, it starts and ends completions arbitarily. That is a suggestion could be 'tensorfl' and not the complete identifier 'tensorflow' which can be a little annoying in a real usage scenario. We can limit them to finish on end of tokens to fix that. Also you can notice that it completes across operators as well. Increasing the length of the beam search will let it complete longer pieces of code.
+[This](https://github.com/vpj/python_autocomplete) a toy project we started
+to see how well a simple LSTM model can autocomplete python code.
+
+It gives quite decent results by saving above 30% key strokes in most files,
+and close to 50% in some.
+We calculated key strokes saved by making a single (best)
+prediction and selecting it with a single key.
+
+We do a beam search to find predictions, upto ~10 characters ahead.
+So far it's too inefficient, if you are wondering about editor integration.
+
+We train and predict on after cleaning comments, strings
+and blank lines in python code.
+The model is trained after tokenizing python code.
+It seems more efficient than character level prediction with byte-pair encoding.
+
+A saved model is included in this repo.
+It is trained on [tensorflow/models](https://github.com/tensorflow/models).
+
+Here's a sample evaluation on a source file from validation set.
+Green characters are when a auto-completion started;
+i.e. user presses TAB to select the completion. 
+The green character and and the following characters highlighted in gray
+are auto-completed. As you can see, it starts and ends completions arbitrarily.
+That is a suggestion could be 'tensorfl' and not the complete identifier
+'tensorflow' which can be a little annoying in a real usage scenario.
+We can limit them to finish on end of tokens to fix that.
+Also you can notice that it completes across operators as well.
+Increasing the length of the beam search will let it complete longer pieces of code.
 
 <p align="center">
   <img src="/python-autocomplete.png?raw=true" width="100%" title="Screenshot">
diff --git a/requirements.txt b/requirements.txt
index e69de29..f039373 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -0,0 +1,3 @@
+machine_learning_lab
+torch
+numpy
diff --git a/train.py b/train.py
index 590c4a7..5beb0cf 100644
--- a/train.py
+++ b/train.py
@@ -4,24 +4,19 @@ from typing import List
 import numpy as np
 import torch
 import torch.nn
+from lab import experiment, monit, tracker, loop, logger
+from lab.utils.delayed_keyboard_interrupt import DelayedKeyboardInterrupt
 
 import parser.load
-from lab.experiment.pytorch import Experiment
 from model import SimpleLstmModel
 from parser import tokenizer
 
-# Configure the experiment
-
-EXPERIMENT = Experiment(name="simple_lstm",
-                        python_file=__file__,
-                        comment="Simple LSTM",
-                        check_repo_dirty=False,
-                        is_log_python_file=False)
-
-logger = EXPERIMENT.logger
+# Setup the experiment
+experiment.create(name="simple_lstm",
+                  comment="Simple LSTM")
 
 # device to train on
-device = torch.device("cuda:1")
+device = torch.device("cuda:0")
 
 
 def list_to_batches(x, batch_size, batches, seq_len):
@@ -118,9 +113,9 @@ class Trainer:
             loss.backward()
             self.optimizer.step()
 
-            logger.store("train_loss", loss.cpu().data.item())
+            tracker.add("train.loss", loss.cpu().data.item())
         else:
-            logger.store("valid_loss", loss.cpu().data.item())
+            tracker.add("valid.loss", loss.cpu().data.item())
 
 
 def main_train():
@@ -129,13 +124,13 @@ def main_train():
     batch_size = 32
     seq_len = 32
 
-    with logger.section("Loading data"):
+    with monit.section("Loading data"):
         # Load all python files
         files = parser.load.load_files()
         # Split training and validation data
         train_files, valid_files = parser.load.split_train_valid(files, is_shuffle=False)
 
-    with logger.section("Create model"):
+    with monit.section("Create model"):
         # Create model
         model = SimpleLstmModel(encoding_size=tokenizer.VOCAB_SIZE,
                                 embedding_size=tokenizer.VOCAB_SIZE,
@@ -153,14 +148,14 @@ def main_train():
     c0 = torch.zeros((lstm_layers, batch_size, lstm_size), device=device)
 
     # Setup logger indicators
-    logger.add_indicator("train_loss", queue_limit=500, is_histogram=True)
-    logger.add_indicator("valid_loss", queue_limit=500, is_histogram=True)
+    tracker.set_queue("train.loss", queue_size=500, is_print=True)
+    tracker.set_queue("valid.loss", queue_size=500, is_print=True)
 
     # Specify the model in [lab](https://github.com/vpj/lab) for saving and loading
-    EXPERIMENT.add_models({'base': model})
+    experiment.add_pytorch_models({'base': model})
 
     # Start training scratch (step '0')
-    EXPERIMENT.start_train(0)
+    experiment.start()
 
     # Number of batches per epoch
     batches = math.ceil(sum([len(f[1]) + 1 for f in train_files]) / (batch_size * seq_len))
@@ -169,7 +164,7 @@ def main_train():
     steps_per_epoch = 200
 
     # Train for 100 epochs
-    for epoch in logger.loop(range(100)):
+    for epoch in loop.loop(range(100)):
         # Create trainer
         trainer = Trainer(files=train_files,
                           model=model,
@@ -199,46 +194,44 @@ def main_train():
 
         # Loop through steps
         for i in range(1, steps_per_epoch):
-            # Set global step
-            global_step = epoch * batches + min(batches, (batches * i) // steps_per_epoch)
-            logger.set_global_step(global_step)
-
-            # Last batch to train and validate
-            train_batch_limit = trainer.x.shape[0] * min(1., (i + 1) / steps_per_epoch)
-            valid_batch_limit = validator.x.shape[0] * min(1., (i + 1) / steps_per_epoch)
-
             try:
-                with logger.delayed_keyboard_interrupt():
+                with DelayedKeyboardInterrupt():
+                    # Set global step
+                    global_step = epoch * batches + min(batches, (batches * i) // steps_per_epoch)
+                    loop.set_global_step(global_step)
 
-                    with logger.section("train", total_steps=trainer.x.shape[0], is_partial=True):
+                    # Last batch to train and validate
+                    train_batch_limit = trainer.x.shape[0] * min(1., (i + 1) / steps_per_epoch)
+                    valid_batch_limit = validator.x.shape[0] * min(1., (i + 1) / steps_per_epoch)
+
+                    with monit.section("train", total_steps=trainer.x.shape[0], is_partial=True):
                         model.train()
                         # Train
                         while train_batch < train_batch_limit:
                             trainer.run(train_batch)
-                            logger.progress(train_batch + 1)
+                            monit.progress(train_batch + 1)
                             train_batch += 1
 
-                    with logger.section("valid", total_steps=validator.x.shape[0], is_partial=True):
+                    with monit.section("valid", total_steps=validator.x.shape[0], is_partial=True):
                         model.eval()
                         # Validate
                         while valid_batch < valid_batch_limit:
                             validator.run(valid_batch)
-                            logger.progress(valid_batch + 1)
+                            monit.progress(valid_batch + 1)
                             valid_batch += 1
 
                     # Output results
-                    logger.write()
+                    tracker.save()
 
                     # 10 lines of logs per epoch
                     if (i + 1) % (steps_per_epoch // 10) == 0:
-                        logger.new_line()
-
+                        logger.log()
             except KeyboardInterrupt:
-                logger.save_progress()
-                logger.save_checkpoint()
-                logger.new_line()
+                experiment.save_checkpoint()
                 return
 
+        experiment.save_checkpoint()
+
 
 if __name__ == '__main__':
     main_train()